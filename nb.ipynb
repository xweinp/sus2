{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac92f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import gymnasium\n",
    "import plotly.express as px\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from utils import Model, ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9acfbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Input dimension: 4, Output dimension: 2\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gymnasium.make(\"CartPole-v1\")\n",
    "\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "max_reward = 500\n",
    "\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Input dimension: {input_dim}, Output dimension: {output_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad1e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_device(dict):\n",
    "    if device == 'cpu' or dict is None:\n",
    "        return dict\n",
    "    return {k: v.to(device) for k, v in dict.items()}\n",
    "\n",
    "# returns X, y, actions\n",
    "def terms_batch(terms):\n",
    "    return terms['state'], terms['reward'], terms['action']\n",
    "\n",
    "# returns X, y, actions\n",
    "def non_terms_batch(non_terms, target_model, gamma):\n",
    "    max_q = target_model.get_max_q(non_terms['next_state'])\n",
    "    y = non_terms['reward'] + gamma * max_q\n",
    "    return non_terms['state'], y, non_terms['action']\n",
    "\n",
    "def process_batch(\n",
    "    batch, \n",
    "    gamma, \n",
    "    target_model,\n",
    "    policy_model, \n",
    "    criterion, \n",
    "    optimizer\n",
    "):\n",
    "    policy_model.train()\n",
    "\n",
    "    non_terms = dict_to_device(batch['non_terms'])\n",
    "    terms = dict_to_device(batch['terms'])\n",
    "\n",
    "    if terms is None:\n",
    "        x, y, actions = non_terms_batch(non_terms, target_model, gamma)\n",
    "\n",
    "    elif non_terms is None:\n",
    "        x, y, actions = terms_batch(terms)\n",
    "    \n",
    "    else:\n",
    "        x_t, y_t, actions_t = terms_batch(terms)\n",
    "        x_nt, y_nt, actions_nt = non_terms_batch(non_terms, target_model, gamma)\n",
    "        \n",
    "        y = torch.cat([y_t, y_nt])\n",
    "        actions = torch.cat([actions_t, actions_nt])\n",
    "        x = torch.cat([x_t, x_nt])\n",
    "    \n",
    "    \n",
    "    preds = policy_model.get_action_qs(x, actions)\n",
    "    loss = criterion(preds, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # I clip the gradient to prevent radical changes in the model\n",
    "    nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bb23b",
   "metadata": {},
   "source": [
    "| Num | Observation           | Min                          | Max                          |\n",
    "|-----|------------------------|-------------------------------|-------------------------------|\n",
    "| 0   | Cart Position          | -4.8                          | 4.8                           |\n",
    "| 1   | Cart Velocity          | -Inf                          | Inf                           |\n",
    "| 2   | Pole Angle             | ~ -0.418 rad (-24°)           | ~ 0.418 rad (24°)             |\n",
    "| 3   | Pole Angular Velocity  | -Inf                          | Inf                           |\n",
    "\n",
    "\n",
    "The cart x-position (index 0) can be take values between (-4.8, 4.8), but the episode terminates if the cart leaves the (-2.4, 2.4) range.\n",
    "\n",
    "The pole angle can be observed between (-.418, .418) radians (or ±24°), but the episode terminates if the pole angle is not in the range (-.2095, .2095) (or ±12°)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e8008",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "\n",
    "Since the goal is to keep the pole upright for as long as possible, by default, a reward of +1 is given for every step taken, including the termination step. The default reward threshold is 500 for v1 and 200 for v0 due to the time limit on the environment.\n",
    "\n",
    "If sutton_barto_reward=True, then a reward of 0 is awarded for every non-terminating step and -1 for the terminating step. As a result, the reward threshold is 0 for v0 and v1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa080880",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "n_episodes = 300\n",
    "eps_greedy = 0.1\n",
    "eps_decay = 0.95\n",
    "gamma = 0.99\n",
    "update_target_steps = 100\n",
    "quality_check_freq = 25\n",
    "\n",
    "hidden_dim = 32\n",
    "\n",
    "target_model = Model(input_dim, hidden_dim, output_dim).to(device)\n",
    "policy_model = Model(input_dim, hidden_dim, output_dim).to(device)\n",
    "replay_memory = ReplayMemory(int(1e4))\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, \n",
    "    start_factor=1.0, \n",
    "    end_factor=0.1, \n",
    "    total_iters=n_episodes\n",
    ")\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5c387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ae17fa8b69495e882bd0c7e21f4d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Episode Lengths',\n",
       "              'type': 'scatter',\n",
       "              'uid': '20eee492-a8b7-434e-8cfb-b949f5d07384',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Episode Lengths'},\n",
       "               'xaxis': {'title': {'text': 'Episode'}},\n",
       "               'yaxis': {'title': {'text': 'Length'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 224/300 [02:14<00:45,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward reached: 500 at episode 226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def torch_state(state):\n",
    "    return torch.tensor(\n",
    "        state,\n",
    "        dtype=dtype\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "def should_random(eps_greedy):\n",
    "    return torch.empty(1).uniform_(0.0, 1.0) < eps_greedy\n",
    "def random_action():\n",
    "    return torch.randint(low=0, high=2, size=(1,))  # [0, 1]\n",
    "\n",
    "fig = go.FigureWidget()\n",
    "fig.add_scatter(x=[], y=[], mode='lines+markers', name='Episode Lengths')\n",
    "fig.update_layout(title='Episode Lengths', xaxis_title='Episode', yaxis_title='Length')\n",
    "display(fig)\n",
    "\n",
    "episode_lens = []\n",
    "smooth = []\n",
    "sm = 0\n",
    "update_it = 0\n",
    "\n",
    "\n",
    "for episode in tqdm(range(1, n_episodes + 1)):\n",
    "    finished = False\n",
    "    \n",
    "    state, info = env.reset()\n",
    "    state = torch_state(state)\n",
    "\n",
    "    episode_len = 0\n",
    "\n",
    "    while not finished:\n",
    "        episode_len += 1\n",
    "\n",
    "        if should_random(eps_greedy):\n",
    "            action = random_action()\n",
    "        else:\n",
    "            policy_model.eval()\n",
    "            state_c = state.to(device)\n",
    "            action = policy_model.predict(state_c).to('cpu')\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        \n",
    "        next_state = torch_state(next_state)\n",
    "        reward = torch.tensor([reward], dtype=dtype).unsqueeze(0)\n",
    "        action = action.unsqueeze(0)\n",
    "\n",
    "        finished = finished or terminated or truncated\n",
    "        if finished:\n",
    "            next_state = None\n",
    "\n",
    "        replay_memory.add({\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'next_state': next_state\n",
    "        })\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if len(replay_memory) < batch_size:\n",
    "            continue\n",
    "        \n",
    "        batch = replay_memory.sample(batch_size)\n",
    "        loss = process_batch(\n",
    "            batch,\n",
    "            gamma,\n",
    "            target_model,\n",
    "            policy_model,\n",
    "            criterion,\n",
    "            optimizer\n",
    "        )\n",
    "\n",
    "        update_it += 1\n",
    "        if update_it == update_target_steps:\n",
    "            target_model.load_state_dict(policy_model.state_dict())\n",
    "            update_it = 0\n",
    "\n",
    "    episode_lens.append(episode_len)\n",
    "    sm += episode_len\n",
    "    \n",
    "    if len(replay_memory) >= batch_size:\n",
    "        eps_greedy *= eps_decay\n",
    "        scheduler.step()\n",
    "    \n",
    "    \n",
    "    if episode % quality_check_freq == 0:\n",
    "        sm /= quality_check_freq\n",
    "        smooth.append(sm)\n",
    "        fig.data[0].x = list(range(\n",
    "            quality_check_freq, \n",
    "            quality_check_freq * (len(smooth) + 1), \n",
    "            quality_check_freq\n",
    "        ))\n",
    "        fig.data[0].y = smooth\n",
    "\n",
    "        if sm == max_reward:\n",
    "            print(f\"Max reward reached: {max_reward} at episode {episode}\")\n",
    "            break\n",
    "\n",
    "        sm = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cef08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_model, 'best_agent.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
