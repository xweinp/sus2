{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac92f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import gymnasium\n",
    "import plotly.express as px\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "\n",
    "dtype = torch.float32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acfbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make(\"CartPole-v1\")\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "print(f\"Input dimension: {input_dim}, Output dimension: {output_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6599b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.mod = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mod(x)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, x):\n",
    "        return self.mod(x).argmax(dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_batch(\n",
    "        batch, \n",
    "        gamma, \n",
    "        target_model,\n",
    "        policy_model, \n",
    "        criterion, \n",
    "        optimizer\n",
    "    ):\n",
    "    policy_model.train()\n",
    "\n",
    "    non_terms = batch['non_terms']\n",
    "    terms = batch['terms']\n",
    "\n",
    "    if terms is not None:\n",
    "        terms = {k: v.to(device) for k, v in terms.items()}\n",
    "    if non_terms is not None:\n",
    "        non_terms = {k: v.to(device) for k, v in non_terms.items()}\n",
    "\n",
    "    if terms is None:\n",
    "        with torch.no_grad():\n",
    "            max_q = target_model(non_terms['next_state']).max(dim=1, keepdim=True).values\n",
    "        y_true = non_terms['reward'] + gamma * max_q\n",
    "        actions = non_terms['action']\n",
    "        X = non_terms['state']\n",
    "    elif non_terms is None:\n",
    "        y_true = terms['reward']\n",
    "        actions = terms['action']\n",
    "        X = terms['state']\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            max_q = target_model(non_terms['next_state']).max(dim=1, keepdim=True).values\n",
    "        y_true_terms = terms['reward']\n",
    "        y_true_nonterms = non_terms['reward'] + gamma * max_q\n",
    "        \n",
    "        y_true = torch.cat([y_true_terms, y_true_nonterms])\n",
    "        actions = torch.cat([terms['action'], non_terms['action']])\n",
    "        X = torch.cat([terms['state'], non_terms['state']])\n",
    "    \n",
    "    preds = policy_model.forward(X).gather(index=actions, dim=1)\n",
    "\n",
    "    if preds.shape[1] == 32 or y_true.shape[1] == 32:\n",
    "        pass\n",
    "    loss = criterion(preds, y_true)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90436002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, max_capacity):\n",
    "        self.memory = deque(maxlen=max_capacity)\n",
    "        self.max_capacity = 10000\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def add(self, transition):\n",
    "        if len(self.memory) >= self.max_capacity:\n",
    "            self.memory.popleft()\n",
    "        self.memory.append(transition)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        ln = len(self.memory)\n",
    "        indices = torch.randint(0, ln, (batch_size,))\n",
    "\n",
    "        non_terms = [\n",
    "            self.memory[i] \n",
    "            for i in indices \n",
    "            if self.memory[i]['next_state'] is not None\n",
    "        ]\n",
    "        terms = [\n",
    "            self.memory[i] \n",
    "            for i in indices \n",
    "            if self.memory[i]['next_state'] is None\n",
    "        ]\n",
    "        \n",
    "        def cat(lst, dim=0):\n",
    "            return torch.cat(lst, dim=dim).to(device)\n",
    "\n",
    "        return {\n",
    "            'terms': {\n",
    "                'state': cat([el['state'] for el in terms]),\n",
    "                'action': torch.cat([el['action'] for el in terms]),\n",
    "                'reward': torch.cat([el['reward'] for el in terms]),\n",
    "            } if len(terms) > 0 else None, \n",
    "            'non_terms': {\n",
    "                'state': cat([el['state'] for el in non_terms]),\n",
    "                'action': torch.cat([el['action'] for el in non_terms]),\n",
    "                'reward': torch.cat([el['reward'] for el in non_terms]),\n",
    "                'next_state': cat([el['next_state'] for el in non_terms]),\n",
    "            } if len(non_terms) > 0 else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bb23b",
   "metadata": {},
   "source": [
    "| Num | Observation           | Min                          | Max                          |\n",
    "|-----|------------------------|-------------------------------|-------------------------------|\n",
    "| 0   | Cart Position          | -4.8                          | 4.8                           |\n",
    "| 1   | Cart Velocity          | -Inf                          | Inf                           |\n",
    "| 2   | Pole Angle             | ~ -0.418 rad (-24°)           | ~ 0.418 rad (24°)             |\n",
    "| 3   | Pole Angular Velocity  | -Inf                          | Inf                           |\n",
    "\n",
    "\n",
    "The cart x-position (index 0) can be take values between (-4.8, 4.8), but the episode terminates if the cart leaves the (-2.4, 2.4) range.\n",
    "\n",
    "The pole angle can be observed between (-.418, .418) radians (or ±24°), but the episode terminates if the pole angle is not in the range (-.2095, .2095) (or ±12°)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e8008",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "\n",
    "Since the goal is to keep the pole upright for as long as possible, by default, a reward of +1 is given for every step taken, including the termination step. The default reward threshold is 500 for v1 and 200 for v0 due to the time limit on the environment.\n",
    "\n",
    "If sutton_barto_reward=True, then a reward of 0 is awarded for every non-terminating step and -1 for the terminating step. As a result, the reward threshold is 0 for v0 and v1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa080880",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "n_episodes = 1000\n",
    "eps_greedy = 0.01\n",
    "gamma = 0.99\n",
    "\n",
    "target_model = Model(input_dim, output_dim).to(device)\n",
    "policy_model = Model(input_dim, output_dim).to(device)\n",
    "replay_memory = ReplayMemory(int(1e4))\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_state(state):\n",
    "    return torch.tensor(\n",
    "        state,\n",
    "        dtype=dtype\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "fig = go.FigureWidget()\n",
    "fig.add_scatter(x=[], y=[], mode='lines+markers', name='Episode Lengths')\n",
    "fig.update_layout(title='Episode Lengths', xaxis_title='Episode', yaxis_title='Length')\n",
    "display(fig)\n",
    "\n",
    "update_target_steps = 100\n",
    "update_it = 0\n",
    "\n",
    "episode_lens = []\n",
    "smooth = []\n",
    "sm = 0\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    finished = False\n",
    "    \n",
    "    state, info = env.reset()\n",
    "    state = torch_state(state)\n",
    "\n",
    "    episode_len = 0\n",
    "\n",
    "    while not finished:\n",
    "        episode_len += 1\n",
    "\n",
    "        if torch.empty(1).uniform_(0.0, 1.0) < eps_greedy:\n",
    "            action = torch.randint(low=0, high=2, size=(1,))  # [0, 1]\n",
    "        else:\n",
    "            action = policy_model.predict(state.to(device)).to('cpu')\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        \n",
    "        next_state = torch_state(next_state)\n",
    "        reward = torch.tensor([reward], dtype=dtype).unsqueeze(0)\n",
    "        action = action.unsqueeze(0)\n",
    "\n",
    "        finished = finished or terminated or truncated\n",
    "        if finished:\n",
    "            next_state = None\n",
    "\n",
    "        transition = {\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'next_state': next_state\n",
    "        }\n",
    "        replay_memory.add(transition)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if len(replay_memory) < batch_size:\n",
    "            continue\n",
    "        \n",
    "        batch = replay_memory.sample(batch_size)\n",
    "\n",
    "        loss = process_batch(\n",
    "            batch,\n",
    "            gamma,\n",
    "            target_model,\n",
    "            policy_model,\n",
    "            criterion,\n",
    "            optimizer\n",
    "        )\n",
    "\n",
    "\n",
    "        update_it += 1\n",
    "        if update_it == update_target_steps:\n",
    "            target_model.load_state_dict(policy_model.state_dict())\n",
    "            update_it = 0\n",
    "\n",
    "    episode_lens.append(episode_len)\n",
    "    \n",
    "    sm += episode_len\n",
    "    if episode >= 10:\n",
    "        sm -= episode_lens[episode - 10]\n",
    "    sm /= min(episode + 1, 10)\n",
    "    smooth.append(sm)\n",
    "    \n",
    "    fig.data[0].x = list(range(1, episode + 2))\n",
    "    fig.data[0].y = smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b7847",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = replay_memory.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_batch(\n",
    "    batch,\n",
    "    gamma,\n",
    "    target_model,\n",
    "    policy_model,\n",
    "    criterion,\n",
    "    optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db98ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['non_terms']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
